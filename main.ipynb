{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f3126",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.12.6)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/jalal/OneDrive/Desktop/AI project/StockCryptoMarketPrediction/venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Initializing Libraries\n",
    "import os\n",
    "import gym\n",
    "import praw\n",
    "import logging\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.inference import BeliefPropagation\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "# Ensure NLTK downloads and paths are set\n",
    "# nltk.data.path.append('/Users/ahadhasanli/nltk_data')  # Add the correct NLTK path\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "\n",
    "# Creating class\n",
    "class FinancialDataPipeline:\n",
    "    # Define initial variables\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=config[\"reddit_client_id\"],\n",
    "            client_secret=config[\"reddit_client_secret\"],\n",
    "            user_agent=config[\"reddit_user_agent\"]\n",
    "        )\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    def fetch_reddit_headlines(self, subreddit_name, limit, save_path):\n",
    "        print(f\"Fetching {limit} headlines from r/{subreddit_name}...\")\n",
    "        subreddit = self.reddit.subreddit(subreddit_name)\n",
    "        headlines = []\n",
    "        for post in subreddit.hot(limit=limit):\n",
    "            headlines.append({'title': post.title, 'score': post.score})\n",
    "\n",
    "        df = pd.DataFrame(headlines)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        df.to_csv(f\"{save_path}/{subreddit_name}_headlines.csv\", index=False)\n",
    "        print(f\"Headlines saved at {save_path}/{subreddit_name}_headlines.csv\")\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_headlines(headline):\n",
    "        tokens = word_tokenize(headline)\n",
    "        tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "        tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def analyze_sentiment(self, data_path, save_path):\n",
    "        reddit_data = pd.read_csv(data_path)\n",
    "        reddit_data[\"processed_title\"] = reddit_data[\"title\"].apply(self.preprocess_headlines)\n",
    "        \n",
    "        # Compute sentiment scores\n",
    "        reddit_data[\"sentiment_score\"] = reddit_data[\"processed_title\"].apply(\n",
    "            lambda x: self.sia.polarity_scores(x)[\"compound\"]\n",
    "        )\n",
    "        \n",
    "        reddit_data.to_csv(save_path, index=False)\n",
    "        print(\"Processed sentiment data saved at:\", save_path)\n",
    "        \n",
    "    # Fetch crypto data from Binance using requests\n",
    "    def fetch_crypto_data(self, symbol, interval, start_date, end_date, save_path):\n",
    "        url = \"https://api.binance.com/api/v3/klines\"\n",
    "        params = {\n",
    "            \"symbol\": symbol,\n",
    "            \"interval\": interval,\n",
    "            \"startTime\": int(pd.Timestamp(start_date).timestamp() * 1000),\n",
    "            \"endTime\": int(pd.Timestamp(end_date).timestamp() * 1000),\n",
    "            \"limit\": 1000\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data, columns=[\n",
    "            \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "            \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "            \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"\n",
    "        ])\n",
    "        df = df[[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "        df.rename(columns={\"timestamp\": \"Date\"}, inplace=True)\n",
    "        df.set_index(\"Date\", inplace=True)\n",
    "        df.to_csv(save_path)\n",
    "        logging.info(f\"Crypto data for {symbol} saved at {save_path}\")\n",
    "\n",
    "    def fetch_stock_data(self, ticker, start_date, end_date, save_path):\n",
    "        data = yf.download(ticker, start=start_date, end=end_date)\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            data.columns = data.columns.droplevel(1)\n",
    "        if not os.path.exists(os.path.dirname(save_path)):\n",
    "            os.makedirs(os.path.dirname(save_path))\n",
    "        print(data)\n",
    "        data.to_csv(save_path)\n",
    "        logging.info(f\"Stock data for {ticker} saved at {save_path}\")\n",
    "\n",
    "    def add_technical_indicators(self, data):\n",
    "        # Normalize column names to lowercase\n",
    "        data.columns = [col.lower() for col in data.columns]\n",
    "\n",
    "        # Add technical indicators\n",
    "        data[\"sma_20\"] = data[\"close\"].rolling(window=20).mean()\n",
    "        data[\"ema_20\"] = data[\"close\"].ewm(span=20, adjust=False).mean()\n",
    "        data[\"rsi_14\"] = self.calculate_rsi(data[\"close\"])\n",
    "        short_ema = data[\"close\"].ewm(span=12, adjust=False).mean()\n",
    "        long_ema = data[\"close\"].ewm(span=26, adjust=False).mean()\n",
    "        data[\"macd\"] = short_ema - long_ema\n",
    "        data[\"signal_line\"] = data[\"macd\"].ewm(span=9, adjust=False).mean()\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_rsi(data, window=14):\n",
    "        delta = data.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    def lstm_prediction(self, data, look_back=60):\n",
    "        data_scaled = self.scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "        X, y = [], []\n",
    "        for i in range(look_back, len(data_scaled)):\n",
    "            X.append(data_scaled[i - look_back:i, 0])\n",
    "            y.append(data_scaled[i, 0])\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "        # Splitting data into test and train parts \n",
    "        split = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units=50, return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=1))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "        model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions = self.scaler.inverse_transform(predictions)\n",
    "        y_test_actual = self.scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(y_test_actual, label=\"Actual\")\n",
    "        plt.plot(predictions, label=\"Predicted\")\n",
    "        plt.title(\"LSTM Prediction vs. Actual\")\n",
    "        plt.xlabel(\"Time Steps\")\n",
    "        plt.ylabel(\"Stock Price\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        return model, predictions\n",
    "\n",
    "    def build_bayesian_network(self, data):\n",
    "        # Discretize continuous data into bins\n",
    "        discretizer = KBinsDiscretizer(n_bins=5, encode=\"ordinal\", strategy=\"uniform\")\n",
    "        data_discrete = data.copy()\n",
    "\n",
    "        # Handle missing values\n",
    "        data_discrete.fillna(method=\"ffill\", inplace=True)\n",
    "        data_discrete.fillna(method=\"bfill\", inplace=True)\n",
    "        data_discrete.fillna(0, inplace=True)  # Final fallback for NaNs\n",
    "\n",
    "        # Validate columns before discretization and discretize\n",
    "        columns_to_discretize = [\"rsi_14\", \"close\", \"macd\"]\n",
    "        for col in columns_to_discretize:\n",
    "            if data_discrete[col].isnull().any():\n",
    "                raise ValueError(f\"NaN values found in column {col} after preprocessing.\")\n",
    "            data_discrete[col] = discretizer.fit_transform(data_discrete[[col]]).astype(int)\n",
    "\n",
    "        # Define the Bayesian Network structure\n",
    "        model = BayesianNetwork([(\"rsi_14\", \"close\"), (\"macd\", \"close\")])\n",
    "        model.fit(data_discrete, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "        # Normalize and validate CPDs\n",
    "        for cpd in model.get_cpds():\n",
    "            cpd.normalize()  # Ensures sum of probabilities equals 1\n",
    "            if not np.isclose(cpd.values.sum(axis=0), 1, atol=0.01).all():\n",
    "                logging.error(f\"CPD for {cpd.variable} still not normalized properly.\")\n",
    "                return None  # or handle differently based on your requirements\n",
    "\n",
    "        # Perform inference using Belief Propagation\n",
    "        inference = BeliefPropagation(model)\n",
    "        try:\n",
    "            query_result = inference.query(variables=[\"close\"], evidence={\"rsi_14\": 2}, show_progress=False)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during inference: {e}\")\n",
    "            return None\n",
    "        logging.info(f\"Bayesian Network Query Result: {query_result}\")\n",
    "        return query_result\n",
    "\n",
    "    \n",
    "    def reinforcement_learning(self, env_name=\"CartPole-v1\", episodes=100):\n",
    "        env = gym.make(env_name)\n",
    "        rewards = []\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                done = terminated or truncated\n",
    "            rewards.append(total_reward)\n",
    "        env.close()\n",
    "        # Plot rewards\n",
    "        plt.plot(range(episodes), rewards)\n",
    "        plt.title(f\"Rewards Over {episodes} Episodes in {env_name}\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Reward\")\n",
    "        plt.show()\n",
    "        return rewards\n",
    "\n",
    "    def visualize_data(self, data, columns, title, xlabel, ylabel):\n",
    "        data[columns].plot(figsize=(10, 5), title=title)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_correlation_matrix(self, data, title=\"Correlation Matrix\"):\n",
    "        correlation_matrix = data.corr()\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def backtest_lstm(self, predictions, actual_values):\n",
    "        mse = mean_squared_error(actual_values, predictions)\n",
    "        logging.info(f\"LSTM Mean Squared Error: {mse}\")\n",
    "\n",
    "        # Profitability Calculation (Example: Cumulative Returns)\n",
    "        actual_returns = actual_values[1:] - actual_values[:-1]\n",
    "        predicted_returns = predictions[1:] - predictions[:-1]\n",
    "\n",
    "        profitability = (predicted_returns * actual_returns).sum()\n",
    "        logging.info(f\"LSTM Profitability (Cumulative Returns): {profitability}\")\n",
    "\n",
    "        return mse, profitability\n",
    "\n",
    "    def evaluate_models(self, stock_data):\n",
    "        # LSTM Evaluation\n",
    "        model, predictions = self.lstm_prediction(stock_data[\"close\"])\n",
    "        predictions = predictions.flatten()\n",
    "        actual_values = stock_data[\"close\"].values[-len(predictions):]\n",
    "        lstm_mse, lstm_profit = self.backtest_lstm(predictions, actual_values)\n",
    "\n",
    "        # Bayesian Networks Evaluation\n",
    "        query_result = self.build_bayesian_network(stock_data)\n",
    "        logging.info(f\"Bayesian Network Query Result: {query_result}\")\n",
    "\n",
    "        # Reinforcement Learning Evaluation\n",
    "        rewards = self.reinforcement_learning(env_name=\"CartPole-v1\", episodes=50)\n",
    "        avg_reward = sum(rewards) / len(rewards)\n",
    "        logging.info(f\"Reinforcement Learning Average Reward: {avg_reward}\")\n",
    "\n",
    "        # Comparison of Models\n",
    "        comparison_results = {\n",
    "            \"LSTM\": {\"MSE\": lstm_mse, \"Profitability\": lstm_profit},\n",
    "            \"Bayesian Network\": {\"Example Query\": query_result},\n",
    "            \"Reinforcement Learning\": {\"Average Reward\": avg_reward},\n",
    "        }\n",
    "\n",
    "        return comparison_results\n",
    "\n",
    "    def visualize_comparison(self, comparison_results):\n",
    "        lstm_metrics = comparison_results[\"LSTM\"]\n",
    "        mse_values = [lstm_metrics.get(\"MSE\", 0)]\n",
    "        profitability_values = [lstm_metrics.get(\"Profitability\", 0)]\n",
    "        rl_avg_reward = comparison_results[\"Reinforcement Learning\"].get(\"Average Reward\", 0)\n",
    "\n",
    "        models_mse_profit = [\"LSTM\"]\n",
    "        models_rl = [\"Reinforcement Learning\"]\n",
    "\n",
    "        if any(mse_values):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(models_mse_profit, mse_values, label=\"MSE\", color=\"blue\", alpha=0.6)\n",
    "            plt.title(\"Model MSE Comparison\")\n",
    "            plt.ylabel(\"MSE\")\n",
    "            plt.show()\n",
    "\n",
    "        if any(profitability_values):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(models_mse_profit, profitability_values, label=\"Profitability\", color=\"green\", alpha=0.6)\n",
    "            plt.title(\"Model Profitability Comparison\")\n",
    "            plt.ylabel(\"Profitability\")\n",
    "            plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(models_rl, [rl_avg_reward], label=\"Avg Reward\", color=\"orange\", alpha=0.6)\n",
    "        plt.title(\"Reinforcement Learning Average Reward\")\n",
    "        plt.ylabel(\"Average Reward\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"reddit_client_id\": \"MmsK7PahE1pwyzbLF2wBKQ\",\n",
    "        \"reddit_client_secret\": \"zAFr9cMvdk1TacfbKiuLL2HH5I0X6A\",\n",
    "        \"reddit_user_agent\": \"mmmirvari\",\n",
    "    }\n",
    "    pipeline = FinancialDataPipeline(config)\n",
    "    pipeline.fetch_reddit_headlines(\"stockmarket\", 100, \"data/sentiment/raw\")\n",
    "    # pipeline.analyze_sentiment(\"data/sentiment/raw/stockmarket_headlines.csv\", \"data/sentiment/processed/stockmarket_sentiment.csv\")\n",
    "\n",
    "    pipeline.fetch_stock_data(\"AAPL\", \"2023-01-01\", \"2024-01-01\", \"data/stocks/AAPL.csv\")\n",
    "    stock_data = pd.read_csv(\"data/stocks/AAPL.csv\", index_col=\"Date\", parse_dates=True)\n",
    "    stock_data = pipeline.add_technical_indicators(stock_data)\n",
    "\n",
    "    pipeline.fetch_crypto_data(\"BTCUSDT\", \"1d\", \"2023-01-01\", \"2024-01-01\", \"data/crypto/BTCUSDT.csv\")\n",
    "    crypto_data = pd.read_csv(\"data/crypto/BTCUSDT.csv\", index_col=\"Date\", parse_dates=True)\n",
    "    crypto_data = pipeline.add_technical_indicators(crypto_data)\n",
    "\n",
    "    pipeline.plot_correlation_matrix(stock_data, title=\"Correlation Matrix of AAPL Features\")\n",
    "    pipeline.visualize_data(stock_data, [\"close\", \"sma_20\", \"ema_20\"], \"AAPL Stock with Indicators\", \"Date\", \"Price\")\n",
    "\n",
    "    pipeline.plot_correlation_matrix(crypto_data, title=\"Correlation Matrix of BTCUSDT Features\")\n",
    "    pipeline.visualize_data(crypto_data, [\"close\", \"sma_20\", \"ema_20\"], \"BTCUSDT with Indicators\", \"Date\", \"Price\")\n",
    "\n",
    "    stock_comparison_results = pipeline.evaluate_models(stock_data)\n",
    "    print(\"Stock Comparison Results:\", stock_comparison_results)\n",
    "\n",
    "    crypto_comparison_results = pipeline.evaluate_models(crypto_data)\n",
    "    print(\"Crypto Comparison Results:\", crypto_comparison_results)\n",
    "\n",
    "    pipeline.visualize_comparison(stock_comparison_results)\n",
    "    pipeline.visualize_comparison(crypto_comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f351a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a28c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
